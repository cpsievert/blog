---
title: Developing tools for exploratory visualization
date: '2017-06-13'
slug: developing-tools-for-exploratory-visualization
---

I always love to hear it when my work [enhances the productivity](https://www.youtube.com/watch?v=zMLVjnRZ3dQ&feature=youtu.be&t=10m41s) of other data scientists. After all, most data scientists need to *maximize* time spent formulating, examining, and refining questions about data; and *minimize* time translating those thoughts into machine code. I've [talked](https://cpsievert.github.io/talks/) at [great](http://cpsievert.github.io/talks/20161212a/) [lengths](http://cpsievert.github.io/talks/20161212b/) about how [my work on plotly](https://cpsievert.github.io/plotcon17/talk) fits into this context, which roughly speaking, allows one to leverage the benefits of web-based visualization *without any knowledge of web technologies*. 

[If](https://www.nytimes.com/interactive/2014/upshot/mapping-the-spread-of-drought-across-the-us.html) [you](https://www.nytimes.com/interactive/2014/04/24/upshot/facebook-baseball-map.html) [keep](https://www.nytimes.com/interactive/2014/upshot/buy-rent-calculator.html) [up](https://www.nytimes.com/2014/04/23/upshot/the-american-middle-class-is-no-longer-the-worlds-richest.html) [with](https://www.nytimes.com/interactive/2014/07/08/upshot/how-the-year-you-were-born-influences-your-politics.html) [online](https://www.nytimes.com/interactive/2014/06/05/upshot/how-the-recession-reshaped-the-economy-in-255-charts.html) [media](https://www.nytimes.com/interactive/2014/11/04/upshot/senate-maps.html), the benefits of interactive web graphics are pretty clear: they can help grab the audience's attention, enchance knowledge transfer, and allow the audience to further investigate detailed information. And yes, assuming you're a web developer, we already have [awesome](https://d3js.org/) [tools](https://github.com/plotly/plotly.js/) for creating web-based data visualizations; but even if you're a [polyglot data science unicorn](https://www.slideshare.net/ryanorban/bridging-the-gap-between-data-science-engineer-building-highperformance-teams/5?src=clipshare), you still shouldn't be writing JavaScript/JSON to do exploratory analysis (where the most useful view of the data is not yet known). That's because [data exploration requires non-linear iteration between data transformation/modeling/visualization](http://r4ds.had.co.nz/explore-intro.html) (which a web browser was not designed to do). That being said, [interactive graphics are certainly useful for exploration](https://plotcon17.cpsievert.me/talk/#9)[^1], which is why we've seen a recent explosion in R/Python/Julia interfaces to JavaScript graphing libraries.

[^1]: Assuming that the time spent iterating from one visualization to the next is relatively small.

I feel very fortunate to have worked (at least part-time) on [the R interface](https://plotly-book.cpsievert.me/) to [plotly.js](https://github.com/plotly/plotly.js) for over 2 years now. I also feel very fortunate to be in a position where the underlying library maintainers respond to my [bug reports and feature requests](https://github.com/plotly/plotly.js/issues?utf8=%E2%9C%93&q=is%3Aissue%20author%3Acpsievert%20). Now that I'm [graduated](/cv.pdf), I've had some time to reflect on these experiences, and think a bit more generally about data science software development and services I can offer [potential clients](/consulting).

Designing data science software often requires making hard decisions about which abstractions to make, and perhaps more importantly, which abstractions *not* to make (especially with respect to visualization). Doing this well requires an intimate knowledge of the most frustating/difficult tasks for most users which means data science software developers should: 
  
  1. Be familiar with the vast ecosystem of existing tools or else we risk re-inventing the wheel.[^2]
  1. Get our hands dirty analyzing data, and applying our tools to real problems, or else we risk optimizing for the wrong problem.
  
[^2]: That isn't to say that "re-inventing the wheel" can't be a good thing, especially when it leads to a better wheel. In my opinion, especially in academia, there is too much of a focus on whether tool(s) *exist*, and not nearly enough attention is paid to whether they are *usable*.
  
This is why I spend a good chunk of my time researching the constantly expanding ecosystem of R packages and using some of those tools to do real data analysis. One important thing I've learned over time is that even the best interfaces do about 80% of what you'd like them to do. That is especially true of visualization software since the scope of possibilities is so large. Fortunately, R packages are free and open-source, meaning that we are (usually) free to modify the software to fit the use case. However, this can be a time-consuming and ultimately futile effort without an intimate understanding of the software. I run into these scenarios with other people's software pretty frequently, but probably even more so with my own software (e.g., the [new linking framework in plotly](https://plotly-book.cpsievert.me/linking-views-without-shiny.html) was motivated by [pedestrians](/software/#pedestrains)). As a result, I am confident in my ability to rapidly prototype interactive data visualizations, and customize them to fit a particular use case if an existing solution doesn't already exist. 




<!--
As someone with a [formal training in statistics](/cv.pdf) who picked up computing/programming relatively informally, I know all too well how easy it is for [statistical thinking to be swamped by unessential & tedious programming tasks](https://plotcon17.cpsievert.me/talk/#5). 

Moreover, expecting data scientists to be polyglot programmers can be harmful: it steals away cognitive effort that could be better spent learning new mathematics/statistical methods, gaining domain expertise, and also contributes to [this unicorn mentality](http://www.rwjf.org/en/library/infographics/infographic-the-myth-of-the-data-scientist-unicorn.html) that the field suffers from. However, the fact remains, if data scientists want to effectively *do* data analysis and *communicate* their results, they need to juggle a myriad of technologies (R, Python, JavaScript, HTML, SVG, etc).

This is especially true if you need to *do* data analysis, then *present* your results to others in a dynamic/interactive web-based format. That is, shifting from the exploratory phase of an analysis to the communication phase can involve an expensive context shift between tools (e.g. R/Python for exploration, JavaScript/D3 for visualization). Moreover, most web-based tools simply weren't designed for fast iteration time *between* common data visualizations, which is a critical

-->

  
  <!--
  I feel very fortunate to have spent numerous years of my statistics PhD in this domain learing from mentors that have been this sort of work for 30+ years. They helped me not to forget the importance of 1 and 2
  
  instilled in me how important it is to not become hyper-focused on software engineering problems
  -->
